

# 初步设想

（仅为娱乐所写，请各位大佬勿喷）

所以我想设计一个压缩算法，但可能不是传统的压缩方法，而是利用哈希碰撞来减少存储空间。解压的时候，通过暴力碰撞来恢复原始数据。用多次哈希计算来验证是否正确碰撞。



## 初步思路

我的初步思路是用SHA256取摘要，解压靠碰撞。将数据分成块，每个块生成一个哈希，存储哈希而不是原始数据。解压时，根据哈希值暴力生成所有可能的块数据，直到找到匹配的哈希。然后通过两三次哈希验证是否碰撞正确，减少误判的概率。例如，先计算SHA256，如果找到碰撞，再用另一个哈希函数（比如SHA3）验证，确保数据无损正确。

假设存储的是256位的哈希，如果块很小，比如每个块是32字节，那么哈希和块的大小相同，没有任何压缩效果。所以，要提高压缩率就要使数据块尽量大，但是又保持在能发挥出并行计算优势的大小范围内。

### 压缩阶段

首先先进行数据分块。
将原始数据切割为固定大小的块（例如 128字节/块），块大小需满足块大小 > 哈希总长度（如用双SHA256则需块>64字节）
若数据末尾不足一块，保留原始字节不切割。

然后再进行双重哈希计算，对每个块计算两个独立哈希值（例如 SHA256 + SHA3-256），形成 `<H1, H2> `组合。

最后再元数据存储，保存所有哈希对及块顺序，压缩文件结构示例：

```
[块1_H1][块1_H2][块2_H1][块2_H2]...[末尾原始字节]
```

### 解压阶段

逐块暴力碰撞，对每个哈希对 `<H1, H2>`，遍历所有可能的块候选值。
对每个候选块计算其H1'和H2'，若同时满足：
`H1' == H1 且 H2' == H2`
则判定为正确块。
最后末尾处理，直接读取末尾未切割的原始字节。

### 回顾发现问题

如果每个块生成两个256位的哈希，即64字节，那么块的大小必须大于64字节才能有压缩效果，这样压缩率才能达到50%。但我们进行可行性边界分析发现

| 块大小 | 哈希存储量 |  候选空间规模  |  理论解压时间  |
| :----: | :--------: | :------------: | :------------: |
| 64字节 |   64字节   | 2^(8×64)=1e154 | 远超出宇宙年龄 |

**理论可行但实际不可行**：SHA256的抗碰撞性导致候选空间规模指数爆炸。

如果将数据分解为位，每个位用哈希表示，解压时尝试0和1，验证哈希。虽然每个位的哈希存储会很大，但解压时只需要两次尝试每个位，这样总时间是线性的。但这样存储每个位的哈希会导致数据膨胀，例如每个位存储32字节的哈希，导致数据膨胀256倍，显然不可行。经过计算我们可以得出结论：

**仅适用于极小块**：≤8字节，但此时哈希存储反超原始数据，失去压缩意义。

因此，这种方法**在理论上可能成立，但在实际中无法实施**，除非存在某种数据冗余或结构，使得候选块的生成不需要遍历所有可能。

**综上，到这里已经可以宣布这个算法的死亡了**。但是基于学习探索的目的，我将会继续完善和改进它。